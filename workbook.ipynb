{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 212)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data\\\\train.data\",sep=\"\\t\")\n",
    "target = pd.read_csv(\"data\\\\train_churn.labels.txt\",  header=None)\n",
    "target.value_counts()\n",
    "df.dropna(how='all', axis=1, inplace=True) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low cardinality : Var191\n",
      "High Cardinality : Var198\n",
      "High Cardinality : Var199\n",
      "High Cardinality : Var200\n",
      "low cardinality : Var201\n",
      "High Cardinality : Var202\n",
      "low cardinality : Var208\n",
      "low cardinality : Var211\n",
      "low cardinality : Var213\n",
      "High Cardinality : Var214\n",
      "low cardinality : Var215\n",
      "High Cardinality : Var216\n",
      "High Cardinality : Var217\n",
      "low cardinality : Var218\n",
      "High Cardinality : Var220\n",
      "High Cardinality : Var222\n",
      "low cardinality : Var224\n"
     ]
    }
   ],
   "source": [
    "numeric= df._get_numeric_data().columns\n",
    "h_cardinal = [(x,df[x].nunique()) for x in df.columns if x not in numeric]\n",
    "for x,y in h_cardinal:\n",
    "    if y >1000 :\n",
    "        print(\"High Cardinality : \"+x)\n",
    "        df.drop(columns= x,inplace =True)\n",
    "    if y <=2:\n",
    "        print(\"low cardinality : \"+ x)\n",
    "        df[x] = pd.factorize(df[x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "# Find numerical and categorical columns\n",
    "numeric_features = df.select_dtypes(include=[np.float64]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# Impute missing values for numerical features\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df[numeric_features] = imputer.fit_transform(df[numeric_features])\n",
    "df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "\n",
    "params=dict(\n",
    "    iterations=1000,\n",
    "                           depth=5,\n",
    "                           learning_rate=0.05,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           l2_leaf_reg=8,\n",
    "                           verbose=400,\n",
    "                           bagging_temperature=8,\n",
    "                           border_count=32,\n",
    "                           random_strength=16,\n",
    "                           class_weights=[1, 22],\n",
    "                           random_seed=42\n",
    ")\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "\n",
    "def cross_validate_fold(model, X, y, train_idx, test_idx, categorical_features):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, cat_features=categorical_features)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    f1_score = evaluate_model(y_test, y_pred)\n",
    "    return f1_score\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform parallelized 10-fold cross-validation\n",
    "f1_scores = Parallel(n_jobs=10)(delayed(cross_validate_fold)(model, df, target, train_idx, test_idx, categorical_features) \n",
    "                                for train_idx, test_idx in kfold.split(df, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-score: 0.20091762311822176\n",
      "Average Precission: 0.11566310233151264\n",
      "Average Recall: 0.7644066165146309\n",
      "0:\tlearn: 0.7772139\ttotal: 202ms\tremaining: 3m 21s\n",
      "400:\tlearn: 0.8124548\ttotal: 25.4s\tremaining: 37.9s\n",
      "800:\tlearn: 0.8511478\ttotal: 53.3s\tremaining: 13.3s\n",
      "999:\tlearn: 0.8611402\ttotal: 1m 7s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x26e1def3920>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Calculate average F1-score\n",
    "avg_f1_score = np.mean([score['1']['f1-score'] for score in f1_scores])\n",
    "print(\"Average F1-score:\", avg_f1_score)\n",
    "avg_precission = np.mean([score['1']['precision'] for score in f1_scores])\n",
    "print(\"Average Precission:\", avg_precission)\n",
    "avg_recall = np.mean([score['1']['recall'] for score in f1_scores])\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"catboost\")\n",
    "mlflow.log_params(params)\n",
    "mlflow.log_metrics({\"F1-Score\":avg_f1_score,\"Precision\": avg_precission,\"Recall\": avg_recall})\n",
    "model.fit(df, target, cat_features=categorical_features)\n",
    "mlflow.catboost.log_model(model, \"catboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "There is no trained model to use save_model(). Use fit() to train model. Then use this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcatboost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\mlflow\\catboost\\__init__.py:253\u001b[0m, in \u001b[0;36mlog_model\u001b[1;34m(cb_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;129m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS\u001b[38;5;241m.\u001b[39mformat(package_name\u001b[38;5;241m=\u001b[39mFLAVOR_NAME))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model\u001b[39m(\n\u001b[0;32m    208\u001b[0m     cb_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    220\u001b[0m ):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Log a CatBoost model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatboost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\mlflow\\models\\model.py:619\u001b[0m, in \u001b[0;36mModel.log\u001b[1;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mfluent\u001b[38;5;241m.\u001b[39m_get_or_start_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[0;32m    618\u001b[0m mlflow_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(artifact_path\u001b[38;5;241m=\u001b[39martifact_path, run_id\u001b[38;5;241m=\u001b[39mrun_id, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[1;32m--> 619\u001b[0m \u001b[43mflavor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# Copy model metadata files to a sub-directory 'metadata',\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# For UC sharing use-cases.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\mlflow\\catboost\\__init__.py:148\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(cb_model, path, conda_env, code_paths, mlflow_model, signature, input_example, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     mlflow_model\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m    147\u001b[0m model_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, _MODEL_BINARY_FILE_NAME)\n\u001b[1;32m--> 148\u001b[0m \u001b[43mcb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m model_bin_kwargs \u001b[38;5;241m=\u001b[39m {_MODEL_BINARY_KEY: _MODEL_BINARY_FILE_NAME}\n\u001b[0;32m    151\u001b[0m pyfunc\u001b[38;5;241m.\u001b[39madd_to_model(\n\u001b[0;32m    152\u001b[0m     mlflow_model,\n\u001b[0;32m    153\u001b[0m     loader_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.catboost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_bin_kwargs,\n\u001b[0;32m    158\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\catboost\\core.py:3399\u001b[0m, in \u001b[0;36mCatBoost.save_model\u001b[1;34m(self, fname, format, export_parameters, pool)\u001b[0m\n\u001b[0;32m   3369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3370\u001b[0m \u001b[38;5;124;03mSave the model to a file.\u001b[39;00m\n\u001b[0;32m   3371\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3396\u001b[0m \u001b[38;5;124;03m    Training pool.\u001b[39;00m\n\u001b[0;32m   3397\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted():\n\u001b[1;32m-> 3399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no trained model to use save_model(). Use fit() to train model. Then use this method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, PATH_TYPES):\n\u001b[0;32m   3401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid fname type=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: must be str() or pathlib.Path().\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(fname)))\n",
      "\u001b[1;31mCatBoostError\u001b[0m: There is no trained model to use save_model(). Use fit() to train model. Then use this method."
     ]
    }
   ],
   "source": [
    "mlflow.catboost.log_model(model, \"catboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "numeric= df._get_numeric_data().columns\n",
    "categoric = [x for x in df.columns if x not in numeric]\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in numeric:\n",
    "        df[col]=df[col].fillna(\"nan\")\n",
    "        df[col]=df[col].astype('category')\n",
    "    if col in numeric:\n",
    "        df[col]=df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but OrdinalEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Var193' 'Var195' 'Var205' 'Var210' 'Var219']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Var7' 'Var28' 'Var46' 'Var65' 'Var68' 'Var72' 'Var73' 'Var74' 'Var81'\n",
      " 'Var91' 'Var100' 'Var102' 'Var112' 'Var113' 'Var121' 'Var126' 'Var136'\n",
      " 'Var137' 'Var138' 'Var152' 'Var160' 'Var164' 'Var175' 'Var185' 'Var191'\n",
      " 'Var201' 'Var213' 'Var215' 'Var218' 'Var224']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model fit using mutual information input features\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test,_col):\n",
    "\toe = OrdinalEncoder()\n",
    "\toe.fit(np.concatenate([X_train, X_test], axis=0))\n",
    "\tX_train_enc = oe.transform(X_train)\n",
    "\tX_test_enc = oe.transform(X_test)\n",
    "\treturn X_train_enc, X_test_enc\n",
    "\n",
    "#OHE\n",
    "def one_hot(X_train, X_test,_col):\n",
    "\t### One hot encoding\n",
    "\tohe = OneHotEncoder(sparse=False)\n",
    "\tohe.fit(np.concatenate([X_train, X_test], axis=0))\n",
    "\tX_train_ohe = ohe.transform(X_train)\n",
    "\tX_test_ohe = ohe.transform(X_test)\n",
    "\treturn X_train_ohe, X_test_ohe\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(y_train)\n",
    "\ty_train_enc = le.transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\treturn y_train_enc, y_test_enc\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test,_col,_k):\n",
    "\tfs = SelectKBest(score_func=mutual_info_classif,k= _k)\n",
    "\tfs.fit(X_train, y_train)\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\tX_test_fs = fs.transform(X_test)\n",
    "\tprint(fs.get_feature_names_out(_col))\n",
    "\treturn X_train_fs, X_test_fs \n",
    "\n",
    "# load the dataset\n",
    "X, y = df.loc[:,categoric],target\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test,X.columns)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs = select_features(X_train_enc, y_train_enc, X_test_enc,X.columns,5)\n",
    "\n",
    " \n",
    "train_rows = X_train.index\n",
    "test_rows = X_test.index\n",
    "X_train_fs_num,X_test_fs_num   = select_features(df.loc[train_rows,numeric],target.iloc[train_rows] ,df.loc[test_rows,numeric],df.loc[:,numeric].columns,30)\n",
    "\n",
    "X_train_fs, X_test_fs = one_hot(X_train_fs, X_test_fs,X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m test_X,test_y \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mconcatenate([X_test_fs,X_test_fs_num],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), y_test \n\u001b[0;32m     12\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m,n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m tsne_result \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m tsne_result\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# (1000, 2)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Two dimensions for each of our images\u001b[39;00m\n\u001b[0;32m     17\u001b[0m  \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Plot the result of our TSNE with the label color coded\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# A lot of the stuff here is about making the plot look pretty and not TSNE\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1001\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[0;32m    999\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1069\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m   1068\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter_without_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_without_progress\n\u001b[1;32m-> 1069\u001b[0m     params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m it\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:402\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[0;32m    400\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 402\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    405\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[1;32mc:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:283\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    280\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    282\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 283\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[0;32m    297\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# We want to get TSNE embedding with 2 dimensions\n",
    "X, y = np.concatenate([X_train_fs,X_train_fs_num],axis=1), y_train_enc\n",
    "test_X,test_y =  np.concatenate([X_test_fs,X_test_fs_num],axis=1), y_test \n",
    "tsne = TSNE(n_components=2,n_jobs=12,perplexity=45,n_iter=1000)\n",
    "tsne_result = tsne.fit_transform(X)\n",
    "tsne_result.shape\n",
    "# (1000, 2)\n",
    "# Two dimensions for each of our images\n",
    " \n",
    "# Plot the result of our TSNE with the label color coded\n",
    "# A lot of the stuff here is about making the plot look pretty and not TSNE\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': y})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X,test_y =  np.concatenate([X_test_fs,X_test_fs_num],axis=1), y_test \n",
    "\n",
    "full_x = np.concatenate([test_X,X])\n",
    "full_y = target\n",
    "\n",
    "inliers = full_x[np.array(target==-1).flatten()]\n",
    "outliers = full_x[np.array(target==1).flatten()]\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "clf = OneClassSVM(gamma='auto').fit(outliers[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([671,   1], dtype=int64))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_out = clf.predict(outliers[3000:])\n",
    "np.unique(y_hat_out, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([46304,    24], dtype=int64))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_in = clf.predict(inliers)\n",
    "np.unique(y_hat_in, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96     11135\n",
      "           1       0.25      0.00      0.00       865\n",
      "\n",
      "    accuracy                           0.93     12000\n",
      "   macro avg       0.59      0.50      0.48     12000\n",
      "weighted avg       0.88      0.93      0.89     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "full_y = (full_y+1)//2\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_x, full_y, test_size=0.24, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': 0.1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "num_round = 500\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Evaluate model\n",
    "threshold = 0.5  # Adjust threshold if necessary\n",
    "y_pred_binary = [1 if p >= threshold else 0 for p in y_pred]\n",
    "\n",
    "report = classification_report(y_true=y_test,y_pred= y_pred_binary)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([17395,  2605], dtype=int64)),\n",
       " (array([0, 1], dtype=int64), array([27796, 27796], dtype=int64)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred_binary,return_counts=True),np.unique(y_train_resampled,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "knn_model = LogisticRegression(max_iter=3000,fit_intercept=True,C=12) # You can change the value of 'k' as needed.\n",
    "\n",
    "knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "y_pred = knn_model.predict(X_test)\n",
    "y_pred_binary = [1 if p >= 0.2 else 0 for p in y_pred]\n",
    "report = classification_report(y_true=y_test,y_pred= y_pred_binary)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'Var8'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7408615\ttotal: 85.6ms\tremaining: 1m 25s\n",
      "400:\tlearn: 0.7623188\ttotal: 24.7s\tremaining: 36.9s\n",
      "800:\tlearn: 0.7932798\ttotal: 53.9s\tremaining: 13.4s\n",
      "999:\tlearn: 0.8078068\ttotal: 1m 8s\tremaining: 0us\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.55      0.70      4640\n",
      "           1       0.12      0.81      0.21       360\n",
      "\n",
      "    accuracy                           0.57      5000\n",
      "   macro avg       0.55      0.68      0.46      5000\n",
      "weighted avg       0.91      0.57      0.67      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = df\n",
    "y = (target+1)//2\n",
    "\n",
    "# Find numerical and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[np.float64]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "# Impute missing values for numerical features\n",
    "imputer = SimpleImputer(strategy='constant',fill_value=0)\n",
    "X[numeric_features] = imputer.fit_transform(X[numeric_features])\n",
    "\n",
    "X[categorical_features] = X[categorical_features].astype(str)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=176)\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "model1 = CatBoostClassifier(iterations=1000,\n",
    "                           depth=5,\n",
    "                           learning_rate=0.03,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           l2_leaf_reg= 8,\n",
    "                           verbose=400,\n",
    "                           bagging_temperature=8,\n",
    "                           border_count=32,\n",
    "                           random_strength=16,\n",
    "                           class_weights=[1, 18],\n",
    "                           random_seed=42)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model1.fit(X_train, y_train, cat_features=categorical_features)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = model1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred1)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([2621, 2379], dtype=int64))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred1,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.12      0.21      4636\n",
      "           1       0.08      0.91      0.14       364\n",
      "\n",
      "    accuracy                           0.18      5000\n",
      "   macro avg       0.51      0.52      0.17      5000\n",
      "weighted avg       0.88      0.18      0.20      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\miniconda3\\envs\\sigmoid\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X[numeric_features], y, test_size=0.24, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "# One-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "column_transformer = ColumnTransformer([('le', OrdinalEncoder(),categorical_features),('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "pre_processing=column_transformer.fit(X)\n",
    "\n",
    "X_train_processed = pre_processing.transform(X_train)\n",
    "X_test_processed = pre_processing.transform(X_test)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "knn_model = LogisticRegression() # You can change the value of 'k' as needed.\n",
    "\n",
    "knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = knn_model.predict_proba(X_test_processed)\n",
    "y_pred_binary = [1 if p[1] >= 0.4 else 0 for p in y_pred]\n",
    "report = classification_report(y_true=y_test,y_pred= y_pred_binary)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.36      0.53      4636\n",
      "           1       0.09      0.85      0.17       364\n",
      "\n",
      "    accuracy                           0.40      5000\n",
      "   macro avg       0.53      0.61      0.35      5000\n",
      "weighted avg       0.90      0.40      0.50      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, np.multiply(y_pred1,y_pred_binary))\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigmoid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
